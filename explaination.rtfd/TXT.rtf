{\rtf1\ansi\ansicpg936\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fswiss\fcharset0 ArialMT;\f1\froman\fcharset0 Times-Roman;\f2\fmodern\fcharset0 CourierNewPSMT;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue255;\red255\green255\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c0\c0\c100000;\cssrgb\c100000\c100000\c100000;
}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl720\sa160\partightenfactor0

\f0\fs53\fsmilli26667 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Perceptron Learning
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 In Python, we firstly handle the original data set datanup_x by adding a list of x(0) = 1 so that we could employ the perceptron learning on data better. Then a for-loop is designed for ensuring the the output after 7000 loops and avoid endless loop. We need to compare the data set of y with the testy calculated by the product of datanup_x and randomw randomly chosen. At the same time, the violations would be add into the violate_indxes list
\fs24 . 
\fs29\fsmilli14667 If each point\'92s testy matches datanup_y, the randomw turns out to be right. Otherwise, we need to make some changes to randomw according to the difference between any violation\'92s testy and datanup_y, at the same time, the new randomw has to be tested again in the for-loop.
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 randomw = [[ 0.02675924][ 5.92127432][-4.89289481][-3.71029569]]
\f1\fs24 \

\f2\fs29\fsmilli14667 #Violations 22\
\pard\pardeftab720\sl400\partightenfactor0

\f1\fs24 \cf2 \
\pard\pardeftab720\sl720\sa160\partightenfactor0

\f0\fs53\fsmilli26667 \cf2 Pocket Algorithm
\f1\b\fs48 \
\pard\pardeftab720\sl280\partightenfactor0

\b0\fs24 \cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 In Python, pocket algorithm is applied to improve perceptron learning by remembering the current best randomw and avoiding the risks of increasing violations led by every change to randomw. In the for-loop, it is necessary to compare the violation caused by new randomw and the remembered best randomw. If the violations have been decreased, we would replace the best randomw with new randomw. Otherwise, we will keep the best randomw unchanged. A result of weights W with step size alpha= 1.6 and max number of iterations=7000 can be:
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 randomw = [[-0.6 \'a0\'a0\'a0\'a0\'a0\'a0][-0.04680538][ 0.06739427][ 0.7663457 ]]
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 A plot on the number of misclassified points (vertical axis) against the number of iterations of algorithm (horizontal axis) can be:
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 violations = [1012, 1007, 1007, 988, 979, 979\'85]
\f1\fs24 \
\pard\pardeftab720\sl400\qc\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 \pard\pardeftab720\sl400\qc\partightenfactor0

\f1\fs24 \cf2 \
\pard\pardeftab720\sl720\sa160\partightenfactor0

\f0\fs53\fsmilli26667 \cf2 Logistic Regression
\f1\b\fs48 \
\pard\pardeftab720\sl280\partightenfactor0

\b0\fs24 \cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 In Python, the aim of logistic regression is to maximize the production of possibilities of dataset, which takes sigmoid function into consideration. In the process, we need to change randomw continuously according to the concept of gradient descent.
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 randomw +=
\f1\fs24 \

\f2\fs29\fsmilli14667 (\cf3 \strokec3 0.1\cf2 \strokec2 /datanup_y.shape[\cf3 \strokec3 0\cf2 \strokec2 ])*np.matrix((\cf3 \strokec3 1\cf2 \strokec2 /(\cf3 \strokec3 1\cf2 \strokec2 +exp(datanup_y[i]*randomw.T*datanup_x[i,:]).T))*datanup_y[i]*(datanup_x[i,:].T).transpose()
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 A result of weights W with step size alpha=0.1 and max number of iterations=7000 can be:
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 randomw = [[ 0.62173953][ 0.36069405][ 0.65464598][ 0.68697845]]\
\pard\pardeftab720\sl400\partightenfactor0

\f1\fs24 \cf2 \
\pard\pardeftab720\sl720\sa160\partightenfactor0

\f0\fs53\fsmilli26667 \cf2 Linear Regression
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 In Python, we are able to obtain easily the randomw by an equation expressed in the class:
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 randomw\cb4  = np.linalg.inv(datanup_x.T * datanup_x) * datanup_x.T * datanup_y
\f1\fs24 \cb1 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 Therefore, it is simple to calculate the randomw in this case:
\f1\fs24 \
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \
\pard\pardeftab720\sl400\partightenfactor0

\f2\fs29\fsmilli14667 \cf2 \cb4 randomw = [ 0.01523535 \'a01.08546357 \'a03.99068855]
\f1\fs24 \cb1 \
}